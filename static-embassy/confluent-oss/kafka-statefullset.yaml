---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {PREFIX}-cp-kafka
  labels:
    app: cp-kafka
    release: {PREFIX}
spec:
  serviceName: {PREFIX}-cp-kafka-headless
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: cp-kafka
        release: {PREFIX}
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - cp-kafka
                  - key: "release"
                    operator: In
                    values:
                    - {PREFIX}
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: cp-kafka-broker
        image: "confluentinc/cp-enterprise-kafka:5.4.1"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 9092
          name: kafka
        resources:
          {}
          
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        - name: HOST_IP
          valueFrom:
            fieldRef:
              fieldPath: status.hostIP
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: POD_NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: KAFKA_HEAP_OPTS
          value: -Xms512M -Xmx512M
        - name: KAFKA_ZOOKEEPER_CONNECT
          value: "{PREFIX}-cp-zookeeper-headless:2181"
        - name: KAFKA_LOG_DIRS
          value: "/opt/kafka/data-0/logs"
        - name: KAFKA_METRIC_REPORTERS
          value: "io.confluent.metrics.reporter.ConfluentMetricsReporter"
        - name: CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS
          value: "PLAINTEXT://{PREFIX}-cp-kafka-headless:9092"
        - name: "KAFKA_LISTENER_SECURITY_PROTOCOL_MAP"
          value: "PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT"
        - name: "KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR"
          value: "3"
        # This is required because the Downward API does not yet support identification of
        # pod numbering in statefulsets. Thus, we are required to specify a command which
        # allows us to extract the pod ID for usage as the Kafka Broker ID.
        # See: https://github.com/kubernetes/kubernetes/issues/31218
        command:
        - sh
        - -exc
        - |
          export KAFKA_BROKER_ID=${HOSTNAME##*-} && \
          export KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://${POD_NAME}.{PREFIX}-cp-kafka-headless.${POD_NAMESPACE}:9092,EXTERNAL://${HOST_IP}:$((31090 + ${KAFKA_BROKER_ID})) && \
          exec /etc/confluent/docker/run
        volumeMounts:
          - name: datadir-0
            mountPath: /opt/kafka/data-0
  volumeClaimTemplates:
  - metadata:
      name: datadir-0
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: "5Gi"
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: {PREFIX}-cp-zookeeper
  labels:
    app: cp-zookeeper
    release: {PREFIX}
spec:
  serviceName: {PREFIX}-cp-zookeeper-headless
  podManagementPolicy: OrderedReady
  replicas: 3
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: cp-zookeeper
        release: {PREFIX}
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                  - key: "app"
                    operator: In
                    values:
                    - cp-zookeeper
                  - key: "release"
                    operator: In
                    values:
                    - {PREFIX}
              topologyKey: "kubernetes.io/hostname"
      containers:
      - name: cp-zookeeper-server
        image: "confluentinc/cp-zookeeper:5.4.1"
        imagePullPolicy: "IfNotPresent"
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: server
        - containerPort: 3888
          name: leader-election
        resources:
          {}
          
        env:
        - name : KAFKA_HEAP_OPTS
          value: "-Xms512M -Xmx512M"
        - name : ZOOKEEPER_TICK_TIME
          value: "2000"
        - name : ZOOKEEPER_SYNC_LIMIT
          value: "5"
        - name : ZOOKEEPER_INIT_LIMIT
          value: "10"
        - name : ZOOKEEPER_MAX_CLIENT_CNXNS
          value: "60"
        - name : ZOOKEEPER_AUTOPURGE_SNAP_RETAIN_COUNT
          value: "3"
        - name : ZOOKEEPER_AUTOPURGE_PURGE_INTERVAL
          value: "24"
        - name: ZOOKEEPER_CLIENT_PORT
          value: "2181"
        - name : ZOOKEEPER_SERVERS
          value: "{PREFIX}-cp-zookeeper-0.{PREFIX}-cp-zookeeper-headless.{NAMESPACE}:2888:3888;{PREFIX}-cp-zookeeper-1.{PREFIX}-cp-zookeeper-headless.{NAMESPACE}:2888:3888;{PREFIX}-cp-zookeeper-2.{PREFIX}-cp-zookeeper-headless.{NAMESPACE}:2888:3888"
        # ZOOKEEPER_SERVER_ID is required just to pass cp-zookeeper ensure script for env check,
        # the value(metadata.mame) is not used and will be overwritten in command part
        - name: ZOOKEEPER_SERVER_ID
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        command:
        - "bash"
        - "-c"
        - |
          ZK_FIX_HOST_REGEX="s/${HOSTNAME}\.[^:]*:/0.0.0.0:/g"
          ZOOKEEPER_SERVER_ID=$((${HOSTNAME##*-}+1)) \
          ZOOKEEPER_SERVERS=`echo $ZOOKEEPER_SERVERS | sed -e "$ZK_FIX_HOST_REGEX"` \
          /etc/confluent/docker/run
        volumeMounts:
        - name: datadir
          mountPath: /var/lib/zookeeper/data
        - name: datalogdir
          mountPath: /var/lib/zookeeper/log
  volumeClaimTemplates:
  - metadata:
      name: datadir
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: "10Gi"
  - metadata:
      name: datalogdir
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: "10Gi"
